{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0b7868",
   "metadata": {},
   "source": [
    "# Classification Performance and Cross Validation\n",
    "\n",
    "Thanks to the `scikit-learn` package, measuring the performance of different machine learning models is simple using the `sklearn.metrics` module (we have used it already when talking about regression!). \n",
    "\n",
    "In this section, we will briefly discuss this module in reference to measuring the performance of classifiers. We will also introduce an additional final aspect essential to machine learning: **cross validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dceb9",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "Similarly to what we have seen for regression, there are various metrics to measure the performance of classifiers. If you want to know about all of the available choices and their detailed description, the best possible source is to directly look into the relative `scikit-learn` documentation page [here](https://scikit-learn.org/stable/modules/model_evaluation.html). \n",
    "\n",
    "Because of its importance and widespread use, and because it effectively summarises different aspects of classifiers, in this note we will focus purely on one of the main metrics contained within the module: the **F-score**. Wikipedia has a very nice article about it with a good graphical illustration [here](https://en.wikipedia.org/wiki/F-score)\n",
    "\n",
    "The F-score, $F_1$ is the harmonic mean of $P$ and $R$, where $P$ is the so-called **precision** of the classifier, and $R$ is the **recall**, defined as:\n",
    "\n",
    "\\begin{align}\n",
    "F_1 &= \\frac{2 P R}{P+R} \\\\\n",
    "R &= \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\\\\\n",
    "P &= \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Note**: This metrics has a few variants, and can be simply called with the function `sklearn.metrics.f1_score`\n",
    "\n",
    "Examining the formula above, it can be seen that the minimum value for $F_1$ is 0, when $\\text{True Positives} = 0$, and it has a maximum value of $F_1=1$ when $(\\text{False Positives} + \\text{False Negatives}) = 0$, in other words, when there are no incorrect predictions.\n",
    "\n",
    "The use of $F_1$ has a metric has received some criticism, in particular for its asymmetric form (if you change the definition of negative vs positive, which is somewhat arbitrary, the score changes!). \n",
    "\n",
    "$F_1$ is not the only important metric that can be used and others might be more relevant, depending on the problem. In particular, the cost of making a wrong prediction might be different depending on whether it is a false positive vs a false negative, and this should be taken into account when evaluating a classifier.\n",
    "\n",
    "> Imagine the case of a covid test and consider the cost for making a false negative vs a false positive prediction before and after working vaccines were developed. What would you consider a potentially better metric in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa833f",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Different metrics used to evaluate a classifier boiled down to different combinations of the fraction of false and true positive and false and true negative. For this reason, regardless of the metric chosen, it is usually quite instructive to visualise this raw data.\n",
    "\n",
    "A confusion matrix is a graphical way to represent this data. In particular, the confusion matrix highlights the amount of wrong vs right predictions, and makes it extremely easy to understand if errors are skewed towards false positive or false negatives. This latter aspect is quite important to potentially improve the classifier and / or understand its limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a1a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class A       1.00      0.67      0.80         3\n",
      "           B       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "\n",
    "y_test = ['A', 'B', 'A', 'B', 'A', 'B']\n",
    "y_pred = ['B', 'B', 'A', 'B', 'A', 'B']\n",
    "target_names = ['class A', 'B']\n",
    "\n",
    "print(sklearn.metrics.classification_report( y_test, y_pred, target_names=target_names) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c10df",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "> **Cross-validation** is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data.</i> (taken from [Amazon Web Services](https://docs.aws.amazon.com/machine-learning/latest/dg/cross-validation.html))\n",
    "\n",
    "Up until this point, we have made a single test-train split, with a single training fraction e.g. `train_ratio = 0.1`. Cross-validation is the act of taking many different test-train splits, and check the results to ensure the consistency of the algorithm chosen. In other words, cross-validation is performed to ensure that a user is picking the best possible model which performs well not just in one particular case, but across many different cases.\n",
    "\n",
    "Said in a different way, the main goal of cross-validation is to prevent **over-fitting** and to gauge how well different strategies such as regularisation, or specific choices of hyper-parameters for the model, prevent this phenomenon. \n",
    "\n",
    "> reminder: over-fitting is when a model performs very well on the data provided to it, but poorly on any new data.\n",
    "\n",
    "The diagram below demonstrates how cross-validation works in practice. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*_7OPgojau8hkiPUiHoGK_w.png\">\n",
    "\n",
    "The steps taken to perform cross-validation are as follows:\n",
    "\n",
    "1. Decide the train-test split parameters \n",
    "2. Choose how many times to do the cross-validation (e.g. 5) \n",
    "3. Choose a model (e.g. `LinearRegression`)\n",
    "4. For each time, use the model to predict data and measure the score (i.e. compare `y_pred` to `y_test`)\n",
    "5. Analyse the mean and standard deviation and compare accross different models.\n",
    "\n",
    "Returning to the previous section of this lecture, Support Vector Machines, it should be clear that the parameters chosen for the kernel function are highly important, as they determine whether or not the data is linearly separable. A common technique for deciding which parameters to use for the kernel function is indeed cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b711563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def create_axes() -> list:\n",
    "    # create axes\n",
    "    fig, ax = plt.subplots(2,2)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax[i][j].axis('off')\n",
    "            ax[i][j].grid('on')\n",
    "    return ax[0][0], ax[0][1], ax[1][0], ax[0][0]\n",
    "\n",
    "def get_training_indices(N: int, ratio: float, seed: int = 100) -> tuple:\n",
    "    \"\"\"Conduct a train test split\"\"\"\n",
    "    # make sure the the ratio is in the right range\n",
    "    assert ratio < 1 and ratio > 0\n",
    "\n",
    "    # reset random seed for repeatability\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # calculate training size which must be an int\n",
    "    training_size = int(N * ratio)\n",
    "    \n",
    "    # create list of all possible indices\n",
    "    indices = range(N)\n",
    "    \n",
    "    # use random.sample(k) to randomly select training indices\n",
    "    training_indices = indices.sample(training_size)\n",
    "    \n",
    "    # get test_indices as well by selecting the indices that\n",
    "    # are not in the training_indices list\n",
    "    test_indices = []\n",
    "    for idx in indices:\n",
    "        if idx not in training_indices:\n",
    "            test_indices.append(idx)\n",
    "        \n",
    "    return training_indices, test_indices\n",
    "\n",
    "def cross_validation_4(X: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Plot 4 different sets of train-test splits\"\"\"\n",
    "    axes = create_axes()\n",
    "    \n",
    "    for i in range(4):\n",
    "        # get training indices\n",
    "        # get test indices\n",
    "                \n",
    "        # make split in X and append to relevant list\n",
    "        # make split in y and append to relevant list\n",
    "        None\n",
    "    return\n",
    "\n",
    "cross_validation_4('','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e202e4",
   "metadata": {},
   "source": [
    "### Cross-validation in `scikit-learn` \n",
    "\n",
    "Thankfully, `scikit-learn` contains a large number of different cross-validation techniques. Here we will only provide a graphical representation, whereas for an exact, accurate explanation we will simply refer to the specific part of the documentation in `scikit-learn` which can be found [here](https://scikit-learn.org/stable/modules/cross_validation.html) and [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html). (this is the end of the course so by now you should have all the tools necessary to understand the documentation of any Python library by yourself!)\n",
    "\n",
    "It is important that you consult these pages and links within it to fully examine how to do the most effective train-test splitting and cross-validation for your particular application.\n",
    "\n",
    "**Please not that these specific sections pointed out by the hyperlinks are considered part of the lecture material!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a1c56",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_004.png\" /></td>\n",
    "        <td><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_005.png\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_006.png\" /></td>\n",
    "        <td><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_007.png\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_008.png\" /></td>\n",
    "        <td><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_009.png\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_010.png\" /></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
