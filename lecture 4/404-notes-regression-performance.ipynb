{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Performance\n",
    "\n",
    "Up until this point, we have covered a few machine learning techniques focused around regression. However, we have not (yet) analysed **how to rank different algorithms in terms of their predictivity**, or how to find the best hyperparameters for a given algorithm. This is what the focus of this section will be on. \n",
    "\n",
    "In this regard, we aim to be quite practical, without focusing too much on the underlying mathematical subtleties of different choices. Instead, we will just quickly show how this is done using `sci-kit learn`.\n",
    "\n",
    "## Metrics\n",
    "\n",
    "**Metrics are the collective term given to the family of analysis techniques used to analyse and measure the performance of different machine learning algorithms**. In short, each metric gives a number (or set of numbers), also called <b>scores</b>, that can be used to measure the performance of a given model. \n",
    "\n",
    "Keep in mind that most metrics are defined in such a way that a higher value means higher predictivity. However, it is always good to check their exact definition, especially to better understand if it fits your specific purpose, which typically depends on details of the problem you are trying to solve.  \n",
    "\n",
    "Here we report some of the metrics used for regression techniques. Although most of them are probably intuitive, you can check find their definitions [here](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce)\n",
    "     \n",
    "- SE - squared error\n",
    "- MSE - mean squared error\n",
    "- RMSE - root mean squared error\n",
    "- rMSE - relative mean squared error\n",
    "- R<sup>2</sup> - coefficient of determination\n",
    "- AE - absolute error\n",
    "- MAE - mean absolute error\n",
    "- Adjusted R<sup>2</sup>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to evaluate a regression algorithm (a short guide):\n",
    "\n",
    "1) **Choose a (few) metric(s)**.<br>\n",
    "Typically, for regression the R<sup>2</sup> or RMSE values are used, and also it is normal for only 1 or 2 of the above metrics to be used to compare different models, and not anymore. However, the right choice of metric is problem specific, and will depend on the reason why you want to deploy your regression model. For example, you might simply want, on average, to make a prediction with the lowest possible error. In other cases, you might accept a larger average error, but want to avoid committing very large errors, even if very few times. \n",
    "\n",
    "2) **Calculate the metric for your algorithm**.<br>\n",
    "Here it is of absolute importance to remember that, in order to check how well an algorithm performs, the metric must be evaluated **on the test set NOT on the training set**.  \n",
    "The reason for using the test set is that an algorithm might have enough parameters to fit exactly any single one of the points that it trains on. However, such accuracy would be useless if it cannot predict the value of other data outside what is has seen in training!\n",
    "\n",
    "> As much as it sounds obvious, this is often not the case and forgotten, especially when you move your first steps into ML!\n",
    "\n",
    "3) **Rank your choice from best to worst, in the order induced by the metric.**\n",
    "Whether you want to choose between different algorithms (say, linear regression vs random forest) or for a certain set of hyperparameters over another (for example, the $L_1$ coefficient in Ridge regression, or the maximum depth of a tree, or the minimum number of datapoints in each leaf in a tree), just see how their predictivity changes using the metric, and **pick the one with the best value...and other application-dependent performances** such as computational costs (i.e., the time or computing resources the algorithm requires to make a prediction).  \n",
    "\n",
    "What do we mean by other \"application-dependent performances\"?  \n",
    "For certain parameters, the best ranking algorithm might also correspond to a model that is  computationally too demanding and takes a long time to return an answer. Thus, you might need to consider performance also in terms of time, or cost per application (especially when this algorithm might be used billions of times each day!). In other words, **the right choice is not always purely a matter of performance metric and a bit of fine tuning might be necessary to find the right compromise.**\n",
    "\n",
    "> **Think:** If algorithm A is marginally better than algorithm B in terms of metric but it takes much longer times to return a prediction, which one would you choose for i) predicting car movement for self-driving cars vs ii) predicting the toxicity dose of a drug?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    explained_variance_score,\n",
    "    max_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different metrics\n",
    "\n",
    "The example below shows how different metrics can be called. The reality is that many of the scores are used in the exact same way, but it is up to the user to choose which metric is the most appropriate for their application.\n",
    "\n",
    "The snippet at the end of the code can be broken down in the following way:\n",
    "\n",
    "1. Loop over a list of the metric functions\n",
    "\n",
    "```python\n",
    "for metric in [\n",
    "    explained_variance_score,\n",
    "    max_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "]:\n",
    "```\n",
    "\n",
    "2. Within a string, print the name of the function\n",
    "\n",
    "```python\n",
    "f'{metric.__name__}'\n",
    "```\n",
    "\n",
    "3. Print the resulting value of the metric, rounded to 2 decimal places - notice how each of the different functions can be called just using the `metric` variable, because it is within the loop.\n",
    "\n",
    "```python\n",
    "f'{metric(y_test, y_pred):.2f}'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric[explained_variance_score] = 0.82\n",
      "Metric[max_error] = 2.41\n",
      "Metric[mean_absolute_error] = 0.22\n",
      "Metric[mean_squared_error] = 0.17\n",
      "Metric[median_absolute_error] = 0.10\n",
      "Metric[r2_score] = 0.82\n"
     ]
    }
   ],
   "source": [
    "# import the data and do the train test split\n",
    "data = pd.read_csv('data tasks/400-fish-preprocessed.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "columns = list(data.columns)\n",
    "y_col = columns.pop(0)\n",
    "y = data[y_col].to_numpy()\n",
    "X = data[columns].to_numpy()\n",
    "training_fraction = 0.1 # we will use 1a0% of the total data to train the model (this is arbitrarily chosen for now)\n",
    "training_size = int(training_fraction * len(X))\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "# create regressor\n",
    "regressor = RandomForestRegressor().fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# analyse metrics\n",
    "for metric in [\n",
    "    explained_variance_score,\n",
    "    max_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "]:\n",
    "    print(f'Metric[{metric.__name__}] = {metric(y_test, y_pred):.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section we have shown how different types of performance metrics that can be used by taking advantage of the `sklearn.metrics` module. These metrics are specific to regression algorithms but other types might be more appropriate for other tasks (e.g., classification), as we shall see in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In Lecture 4 we have covered an introduction into different regression techniques, using the `scikit-learn` package. As an extension of the preprocessing stage, we have shown how data can be split into a training set and a test set, to allow the performance of a model to be analysed. Additionally, we have presented an overview of learning models within `scikit-learn` which use that `.fit` and `.predict` methods to do machine learning on training data, and how their performance can be measured by using the `.score` method or by using the `sklearn.metrics` module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
